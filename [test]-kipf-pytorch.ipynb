{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(index, length):\n",
    "    \"\"\"Return a multi-hot vector with the 1 at specified index. Used to select rows from matrix.\n",
    "    \n",
    "    Args:\n",
    "        index: specified index for the 1 cells.\n",
    "        length: the length of the mask.\n",
    "    Returns:\n",
    "        the multi-hot vector with 0->False, 1->True.\n",
    "    \"\"\"\n",
    "    mask = np.zeros(length)\n",
    "    mask[index] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def parse_index_file(file_path):\n",
    "    \"\"\"Read the index file to get the indices of the test nodes.\"\"\"\n",
    "    indices = []\n",
    "    for line in open(file_path):\n",
    "        indices.append(int(line.strip()))\n",
    "    return indices\n",
    "\n",
    "def load_citeseer(data_dir):\n",
    "    \"\"\"Loads input data from citeseer data directory\n",
    "    \n",
    "    Args:\n",
    "        data_dir: under this folder ...\n",
    "            ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "            ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "            ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "                (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "            ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "            ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "            ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "            ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "                object;\n",
    "            ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "            All objects above must be saved using python pickle module.\n",
    "    Returns: \n",
    "        All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    # Load raw data from files (see Data/ file for details).\n",
    "    names = [\"x\", \"y\", \"tx\", \"ty\", \"allx\", \"ally\", \"graph\"]\n",
    "    objects = []\n",
    "    for name in names:\n",
    "        with open(data_dir + \"ind.citeseer.\" + name, \"rb\") as f:\n",
    "            objects.append(pkl.load(f, encoding=\"latin1\"))\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_index_reorder = parse_index_file(data_dir + \"ind.citeseer.test.index\") # unordered indices.    \n",
    "    test_index_range = np.sort(test_index_reorder) # ordered indices.\n",
    "    # Fix citeseer data by assigning 0 vectors to isolated notes.\n",
    "    test_index_range_full = range(min(test_index_reorder), max(test_index_reorder)+1)\n",
    "    tx_extended = sp.lil_matrix((len(test_index_range_full), x.shape[1])) # linked list based matrix.\n",
    "    tx_extended[test_index_range - min(test_index_range), :] = tx\n",
    "    tx = tx_extended\n",
    "    ty_extended = np.zeros((len(test_index_range_full), y.shape[1]))\n",
    "    ty_extended[test_index_range - min(test_index_range), :] = ty\n",
    "    ty = ty_extended\n",
    "    # Load features\n",
    "    features = sp.vstack((allx, tx)).tolil() # to linked list based matrix.\n",
    "    features[test_index_reorder, :] = features[test_index_range, :] # sort the test nodes related rows.\n",
    "    # Load adjacent matrix\n",
    "    adj_matrix = nx.adjacency_matrix(nx.from_dict_of_lists(graph)) # number_nodes x number_nodes shape.\n",
    "    # Load labels\n",
    "    labels = np.vstack((ally, ty)) # number_nodes x number_classes.\n",
    "    labels[test_index_reorder, :] = labels[test_index_range, :] # sort the test nodes related rows.\n",
    "    # Make mask for row selection\n",
    "    indices_test = test_index_range.tolist()\n",
    "    indices_train = range(len(y))\n",
    "    indices_valid = range(len(y), len(y)+500) # select the first 500 nodes after the labeled nodes as validation.\n",
    "    train_mask = sample_mask(indices_train, labels.shape[0])\n",
    "    valid_mask = sample_mask(indices_valid, labels.shape[0])\n",
    "    test_mask = sample_mask(indices_test, labels.shape[0])\n",
    "    # Make containers for predictions\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_valid = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_valid[valid_mask, :] = labels[valid_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "    \n",
    "    return adj_matrix, features, y_train, y_valid, y_test, train_mask, valid_mask, test_mask \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/jacobsuwang/Documents/UTA/Fall2018/LIN389C/GCN/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj_matrix, features, y_train, y_valid, y_test, train_mask, valid_mask, test_mask = load_citeseer(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj_matrix\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (3327, 3327)\n",
      "features\n",
      "<class 'scipy.sparse.lil.lil_matrix'> (3327, 3703)\n",
      "y_train\n",
      "<class 'numpy.ndarray'> (3327, 6)\n",
      "y_valid\n",
      "<class 'numpy.ndarray'> (3327, 6)\n",
      "y_test\n",
      "<class 'numpy.ndarray'> (3327, 6)\n",
      "train_mask\n",
      "<class 'numpy.ndarray'> (3327,)\n",
      "valid_mask\n",
      "<class 'numpy.ndarray'> (3327,)\n",
      "test_mask\n",
      "<class 'numpy.ndarray'> (3327,)\n"
     ]
    }
   ],
   "source": [
    "def print_info(objects, object_names):\n",
    "    for obj,obj_name in zip(objects,object_names):\n",
    "        print(obj_name)\n",
    "        print(type(obj), obj.shape)\n",
    "\n",
    "objects = [adj_matrix, features, y_train, y_valid, y_test, train_mask, valid_mask, test_mask]\n",
    "object_names = [\"adj_matrix\", \"features\", \"y_train\", \"y_valid\", \"y_test\", \"train_mask\", \"valid_mask\", \"test_mask\"]\n",
    "print_info(objects, object_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_x = pkl.load(open(data_dir+\"ind.citeseer.x\", \"rb\"), encoding=\"latin1\")\n",
    "# sample_tx = pkl.load(open(data_dir+\"ind.citeseer.tx\", \"rb\"), encoding=\"latin1\")\n",
    "# sample_allx = pkl.load(open(data_dir+\"ind.citeseer.allx\", \"rb\"), encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_matrix):\n",
    "    \"\"\"Decompose a sparse matrix into its coordinates, values and shape.\n",
    "    \n",
    "    Args:\n",
    "        sparse_matrix: a scipy.sparse.csr_matrix object.\n",
    "    Returns:\n",
    "        coords: a <number_values, 2> array (2: <x,y> coordinates).\n",
    "        values: the values that fill the coordinates.\n",
    "        shape: the shape of the matrix.\n",
    "    Example:\n",
    "        >> row  = np.array([0, 3, 1, 0])\n",
    "        >> col  = np.array([0, 3, 1, 2])\n",
    "        >> data = np.array([4, 5, 7, 9])\n",
    "        >> csr_sample = sp.csr_matrix((data, (row, col)), shape=(4, 4))\n",
    "        >> csr_sample.toarray()\n",
    "        array([[4, 0, 9, 0],\n",
    "               [0, 7, 0, 0],\n",
    "               [0, 0, 0, 0],\n",
    "               [0, 0, 0, 5]], dtype=int64)\n",
    "        >> sparse_to_tuple(csr_sample)\n",
    "        (array([[0, 0],\n",
    "                [0, 2],\n",
    "                [1, 1],\n",
    "                [3, 3]], dtype=int32), array([4, 9, 7, 5], dtype=int64), (4, 4))\n",
    "    \"\"\"\n",
    "    def to_tuple(matrix):\n",
    "        if not sp.isspmatrix_coo(matrix):\n",
    "            matrix = matrix.tocoo()\n",
    "        coords = np.vstack((matrix.row, matrix.col)).transpose()\n",
    "        values = matrix.data\n",
    "        shape = matrix.shape\n",
    "        return coords, values, shape\n",
    "    if isinstance(sparse_matrix, list): # if is a list of sparse matrices.\n",
    "        for i in range(len(sparse_matrix)):\n",
    "            sparse_matrix[i] = to_tuple(sparse_matrix[i])\n",
    "    else:\n",
    "        sparse_matrix = to_tuple(sparse_matrix)\n",
    "    return sparse_matrix\n",
    "            \n",
    "def to_A_tilde(A):\n",
    "    \"\"\"Add self-connection (Kipf & Welling, 2017, section 2).\"\"\"\n",
    "    return A + sp.eye(A.shape[0])\n",
    "\n",
    "def to_A_hat(A_tilde):\n",
    "    \"\"\"Normalize adjacent matrix (with self-connections), (Kipf & Welling, 2017, section 3.1).\"\"\"\n",
    "    A_tilde = sp.coo_matrix(A_tilde)\n",
    "    rowsum = np.array(A_tilde.sum(axis=1)) # compute diagonal values for D.\n",
    "    D_inv_sqrt = np.power(rowsum, -0.5).flatten() # compute inverse square root of D.\n",
    "    D_inv_sqrt[np.isinf(D_inv_sqrt)] = 0. # convert inf values to 0.\n",
    "    D_inv_sqrt = sp.diags(D_inv_sqrt) # convert vectorized diagonal values to a diagonal matrix.\n",
    "    return A_tilde.dot(D_inv_sqrt).transpose().dot(D_inv_sqrt).tocoo() # formula for A_hat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"Single graph convolution layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_size):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, hidden_size)\n",
    "        \n",
    "    def forward(self, A_hat, X):\n",
    "        return self.linear(A_hat.mm(X))\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"Graph Convolutional Network (Kipf & Welling, 2017, Eq.9).\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_size_1, hidden_size_2):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layer_1 = GraphConvolution(in_features, hidden_size_1)\n",
    "        self.layer_2 = GraphConvolution(hidden_size_1, hidden_size_2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, A_hat, X):\n",
    "        out = self.layer_1(A_hat, X)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(A_hat, out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "def to_categorical(one_hot):\n",
    "    number_classes = one_hot.shape[1]+1\n",
    "    categorical = []\n",
    "    for row in one_hot:\n",
    "        index = np.where(row==1)[0]\n",
    "        if len(index) == 0:\n",
    "            categorical.append([number_classes-1])\n",
    "        else:\n",
    "            categorical.append([index[0]])\n",
    "    return np.array(categorical)\n",
    "\n",
    "    \n",
    "def to_tensor(raw_inputs, tensor_type=torch.FloatTensor):\n",
    "    \"\"\"numpy ndarray or sparse matrix to torch tensor.\"\"\"\n",
    "    if type(raw_inputs) != np.ndarray:\n",
    "        raw_inputs = raw_inputs.toarray()\n",
    "    tensor = Variable(torch.Tensor(raw_inputs).type(tensor_type))\n",
    "    if CUDA:\n",
    "        return tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "TRAIN_FROM, TRAIN_TO = 0, 120\n",
    "VALID_FROM, VALID_TO = 120, 620\n",
    "TEST_FROM, TEST_TO = 2312, 3327\n",
    "    \n",
    "def train_gcn(adj_matrix, features, \n",
    "              y_train, y_valid, y_test,\n",
    "              train_mask, valid_mask, test_mask,\n",
    "              number_iterations=100, print_every=5):\n",
    "    \n",
    "    A_tilde = to_A_tilde(adj_matrix)\n",
    "    A_hat = to_tensor(to_A_hat(A_tilde))\n",
    "    X = to_tensor(features)\n",
    "    \n",
    "    in_features = X.shape[1]\n",
    "    hidden_size_1 = 50\n",
    "    hidden_size_2 = y_train.shape[1]+1 # one more for all-0 rows.    \n",
    "    \n",
    "    y_train = to_tensor(to_categorical(y_train * train_mask[:, np.newaxis]).squeeze())\n",
    "    y_valid = to_tensor(to_categorical(y_valid * valid_mask[:, np.newaxis]).squeeze())\n",
    "    y_test = to_tensor(to_categorical(y_test * test_mask[:, np.newaxis]).squeeze())\n",
    "    number_train = train_mask.sum()\n",
    "    number_valid = valid_mask.sum()\n",
    "    number_test = test_mask.sum()\n",
    "    \n",
    "    gcn = GCN(in_features, hidden_size_1, hidden_size_2)\n",
    "    if CUDA:\n",
    "        gcn = gcn.cuda()\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(gcn.parameters(), lr=1e-4)\n",
    "    \n",
    "    for i in range(number_iterations):\n",
    "        out = gcn(A_hat, X)\n",
    "        # NB: arg1 has type .FloatTensor, arg2 has type .LongTensor.\n",
    "        train_loss = criterion(out, y_train.type(torch.cuda.LongTensor))\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if i != 0 and i % print_every == 0:\n",
    "            print(\"Iteration \" + str(i) + \":\\n\")\n",
    "            print(\"Train loss =\", train_loss.cpu().data.numpy()[0], \"(at step \"+str(i)+\")\")\n",
    "            _, predictions = torch.max(out.data, 1)\n",
    "            number_correct_train = predictions[TRAIN_FROM:TRAIN_TO] \\\n",
    "                .eq(y_train[TRAIN_FROM:TRAIN_TO].type(torch.cuda.LongTensor).data).sum()\n",
    "            number_correct_valid = predictions[VALID_FROM:VALID_TO] \\\n",
    "                .eq(y_valid[VALID_FROM:VALID_TO].type(torch.cuda.LongTensor).data).sum()\n",
    "            number_correct_test = predictions[TEST_FROM:TEST_TO] \\\n",
    "                .eq(y_test[TEST_FROM:TEST_TO].type(torch.cuda.LongTensor).data).sum()      \n",
    "            accuracy_train = number_correct_train / number_train\n",
    "            accuracy_valid = number_correct_valid / number_valid\n",
    "            accuracy_test = number_correct_test / number_test\n",
    "            print(\"Train/Valid/Test accuracy: %.4f | %.4f | %.4f\\n\" % (accuracy_train,\n",
    "                                                                       accuracy_valid,\n",
    "                                                                       accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5:\n",
      "\n",
      "Train loss = 1.9540343 (at step 5)\n",
      "Train/Valid/Test accuracy: 0.2500 | 0.2260 | 0.2290\n",
      "\n",
      "Iteration 10:\n",
      "\n",
      "Train loss = 1.9502562 (at step 10)\n",
      "Train/Valid/Test accuracy: 0.2500 | 0.2200 | 0.2240\n",
      "\n",
      "Iteration 15:\n",
      "\n",
      "Train loss = 1.9460775 (at step 15)\n",
      "Train/Valid/Test accuracy: 0.2417 | 0.2100 | 0.2170\n",
      "\n",
      "Iteration 20:\n",
      "\n",
      "Train loss = 1.9414792 (at step 20)\n",
      "Train/Valid/Test accuracy: 0.2083 | 0.2040 | 0.2120\n",
      "\n",
      "Iteration 25:\n",
      "\n",
      "Train loss = 1.936476 (at step 25)\n",
      "Train/Valid/Test accuracy: 0.1583 | 0.1800 | 0.1700\n",
      "\n",
      "Iteration 30:\n",
      "\n",
      "Train loss = 1.9310786 (at step 30)\n",
      "Train/Valid/Test accuracy: 0.0750 | 0.1120 | 0.1100\n",
      "\n",
      "Iteration 35:\n",
      "\n",
      "Train loss = 1.9252805 (at step 35)\n",
      "Train/Valid/Test accuracy: 0.0333 | 0.0520 | 0.0340\n",
      "\n",
      "Iteration 40:\n",
      "\n",
      "Train loss = 1.9190581 (at step 40)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0120 | 0.0100\n",
      "\n",
      "Iteration 45:\n",
      "\n",
      "Train loss = 1.9123733 (at step 45)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0040 | 0.0020\n",
      "\n",
      "Iteration 50:\n",
      "\n",
      "Train loss = 1.9051807 (at step 50)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0050\n",
      "\n",
      "Iteration 55:\n",
      "\n",
      "Train loss = 1.8974305 (at step 55)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0040\n",
      "\n",
      "Iteration 60:\n",
      "\n",
      "Train loss = 1.8890725 (at step 60)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0120\n",
      "\n",
      "Iteration 65:\n",
      "\n",
      "Train loss = 1.8800569 (at step 65)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0130\n",
      "\n",
      "Iteration 70:\n",
      "\n",
      "Train loss = 1.8703328 (at step 70)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0150\n",
      "\n",
      "Iteration 75:\n",
      "\n",
      "Train loss = 1.8598527 (at step 75)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0150\n",
      "\n",
      "Iteration 80:\n",
      "\n",
      "Train loss = 1.8485689 (at step 80)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0150\n",
      "\n",
      "Iteration 85:\n",
      "\n",
      "Train loss = 1.8364362 (at step 85)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0150\n",
      "\n",
      "Iteration 90:\n",
      "\n",
      "Train loss = 1.8234116 (at step 90)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0150\n",
      "\n",
      "Iteration 95:\n",
      "\n",
      "Train loss = 1.8094541 (at step 95)\n",
      "Train/Valid/Test accuracy: 0.0000 | 0.0000 | 0.0150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_gcn(adj_matrix, features, y_train, y_valid, y_test, train_mask, valid_mask, test_mask,\n",
    "          number_iterations=100, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_matrix\n",
    "# <class 'scipy.sparse.csr.csr_matrix'> (3327, 3327)\n",
    "# features\n",
    "# <class 'scipy.sparse.lil.lil_matrix'> (3327, 3703)\n",
    "# y_train\n",
    "# <class 'numpy.ndarray'> (3327, 6)\n",
    "# y_valid\n",
    "# <class 'numpy.ndarray'> (3327, 6)\n",
    "# y_test\n",
    "# <class 'numpy.ndarray'> (3327, 6)\n",
    "# train_mask\n",
    "# <class 'numpy.ndarray'> (3327,)\n",
    "# valid_mask\n",
    "# <class 'numpy.ndarray'> (3327,)\n",
    "# test_mask\n",
    "# <class 'numpy.ndarray'> (3327,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
